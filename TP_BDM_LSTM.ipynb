{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBj3_Yb633O4",
        "outputId": "d68b9978-3625-4a2e-cda2-9685f0ed7432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BVKCtja6xLJ"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rvSfyvt4NQw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyBNZLajunV9"
      },
      "outputs": [],
      "source": [
        "# Importer les bibliothèques nécessaires\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import SparkSession\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRYfjOyJuqu9"
      },
      "outputs": [],
      "source": [
        "#Initialisation de la session Spark\n",
        "MAX_MEMORY = \"8g\"\n",
        "spark = SparkSession \\\n",
        ".builder \\\n",
        ".appName(\"TP_BDM_LSTM\") \\\n",
        ".config(\"spark.executor.memory\", MAX_MEMORY) \\\n",
        ".config(\"spark.driver.memory\", MAX_MEMORY) \\\n",
        ".getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCmm7ektUMT6"
      },
      "outputs": [],
      "source": [
        "Start = time.time()\n",
        "# Charger le dataset\n",
        "%cd /content/drive/MyDrive/\n",
        "df = spark.read.csv(\"sentiment140.csv\", header=None)\n",
        "# Renommer les colonnes\n",
        "df = df.withColumnRenamed(\"_c0\", \"target\").withColumnRenamed(\"_c5\", \"tweet\")\n",
        "# Convertir les valeurs de la colonne target en 0 et 1\n",
        "df = df.withColumn(\"target\", col(\"target\") / 4)\n",
        "# Convertir le type de la colonne \"target\" en entier\n",
        "df = df.withColumn(\"target\", df[\"target\"].cast(\"integer\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aq4tjqR24Iwl",
        "outputId": "2fe03606-2fa7-4e96-db37-8265cf08404a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "|target|       _c1|                 _c2|     _c3|            _c4|               tweet|\n",
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
            "|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
            "|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
            "|     0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
            "|     0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
            "|     0|1467811372|Mon Apr 06 22:20:...|NO_QUERY|       joy_wolf|@Kwesidei not the...|\n",
            "|     0|1467811592|Mon Apr 06 22:20:...|NO_QUERY|        mybirch|         Need a hug |\n",
            "|     0|1467811594|Mon Apr 06 22:20:...|NO_QUERY|           coZZ|@LOLTrish hey  lo...|\n",
            "|     0|1467811795|Mon Apr 06 22:20:...|NO_QUERY|2Hood4Hollywood|@Tatiana_K nope t...|\n",
            "|     0|1467812025|Mon Apr 06 22:20:...|NO_QUERY|        mimismo|@twittera que me ...|\n",
            "|     0|1467812416|Mon Apr 06 22:20:...|NO_QUERY| erinx3leannexo|spring break in p...|\n",
            "|     0|1467812579|Mon Apr 06 22:20:...|NO_QUERY|   pardonlauren|I just re-pierced...|\n",
            "|     0|1467812723|Mon Apr 06 22:20:...|NO_QUERY|           TLeC|@caregiving I cou...|\n",
            "|     0|1467812771|Mon Apr 06 22:20:...|NO_QUERY|robrobbierobert|@octolinz16 It it...|\n",
            "|     0|1467812784|Mon Apr 06 22:20:...|NO_QUERY|    bayofwolves|@smarrison i woul...|\n",
            "|     0|1467812799|Mon Apr 06 22:20:...|NO_QUERY|     HairByJess|@iamjazzyfizzle I...|\n",
            "|     0|1467812964|Mon Apr 06 22:20:...|NO_QUERY| lovesongwriter|Hollis' death sce...|\n",
            "|     0|1467813137|Mon Apr 06 22:20:...|NO_QUERY|       armotley|about to file taxes |\n",
            "|     0|1467813579|Mon Apr 06 22:20:...|NO_QUERY|     starkissed|@LettyA ahh ive a...|\n",
            "|     0|1467813782|Mon Apr 06 22:20:...|NO_QUERY|      gi_gi_bee|@FakerPattyPattz ...|\n",
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Epoch 1/5\n",
            " 6855/40016 [====>.........................] - ETA: 4:41:05 - loss: 0.5871 - accuracy: 0.7057"
          ]
        }
      ],
      "source": [
        "#Affichage des données\n",
        "df.show()\n",
        "# Diviser les données en ensembles d'apprentissage et de test\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "# Convertir les données Spark DataFrame en pandas DataFrame pour le prétraitement avec Keras\n",
        "train_data_pd = train_data.toPandas()\n",
        "test_data_pd = test_data.toPandas()\n",
        "\n",
        "# Tokeniser les tweets\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' ',\n",
        "    char_level=False,\n",
        "    oov_token=None,\n",
        "    analyzer=None)\n",
        "tokenizer.fit_on_texts(train_data_pd[\"tweet\"])\n",
        "train_sequences = tokenizer.texts_to_sequences(train_data_pd[\"tweet\"])\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data_pd[\"tweet\"])\n",
        "\n",
        "# Remplir les séquences pour qu'elles aient la même longueur\n",
        "max_len = 100 # définir la longueur maximale des séquences\n",
        "train_data = pad_sequences(train_sequences, maxlen=max_len, padding=\"post\")\n",
        "test_data = pad_sequences(test_sequences, maxlen=max_len, padding=\"post\")\n",
        "\n",
        "# Définir l'architecture du modèle LSTM\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32, input_length=max_len))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compiler le modèle\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Entraîner le modèle\n",
        "model.fit(train_data, train_data_pd[\"target\"], epochs=5, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KA_pnWst2_i",
        "outputId": "ec31bc9f-025c-4c3d-ebfd-e1b1b44a1ee2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9985/9985 [==============================] - 54s 5ms/step - loss: 0.4768 - accuracy: 0.8004\n",
            "Test Loss: 0.47679203748703003\n",
            "Test Accuracy: 0.8003516793251038\n",
            "83.71328663825989\n"
          ]
        }
      ],
      "source": [
        "# Évaluer le modèle\n",
        "scores = model.evaluate(test_data, test_data_pd[\"target\"], verbose=1)\n",
        "print(\"Test Loss:\", scores[0])\n",
        "print(\"Test Accuracy:\", scores[1])\n",
        "end = time.time()\n",
        "print(end-Start)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}